{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import pipeline\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a label-to-index mapping (Manually defined or inferred from your data)\n",
    "labels = [\"O\", \"B-organic-chemicals\", \"I-organic-chemicals\", \"B-catalyst\", \"I-catalyst\", \"B-property values\", \"I-property values\"]\n",
    "\n",
    "# Create a dictionary mapping labels to integers\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# Step 2: Convert labels to indices\n",
    "def convert_labels_to_indices(labels, label2id):\n",
    "    return [label2id.get(label, label2id['O']) for label in labels]  # Default to 'O' if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Read and parse the CoNLL file\n",
    "def load_conll_data(filename):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    sentence = []\n",
    "    label = []\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                token, ner_tag = line.split(\"\\t\")  # Assuming space-separated token and label\n",
    "                sentence.append(token)\n",
    "                label.append(ner_tag)\n",
    "            else:\n",
    "                if sentence:  # If sentence is not empty, store it\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                sentence = []\n",
    "                label = []\n",
    "                \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Tokenization function to match BERT's input format\n",
    "def tokenize_and_align_labels(tokenizer, sentences, labels):\n",
    "    tokenized_inputs = tokenizer(sentences, padding=True, truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    labels_all = []\n",
    "    for i, label in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)  # word_ids() is available on fast tokenizers\n",
    "        label_ids = [-100 if word_id is None else label[word_id] for word_id in word_ids]\n",
    "        labels_all.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels_all\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Read the data from file\n",
    "# Assuming your CoNLL file is named 'train.conll'\n",
    "train_sentences, train_labels = load_conll_data(r'ner-chem\\ner\\data.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Convert string labels to indices\n",
    "train_labels_numeric = [convert_labels_to_indices(labels, label2id) for labels in train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Initialize the fast tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Step 8: Create Dataset with tokenized sentences and numeric labels\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"tokens\": train_sentences,  # List of tokenized sentences\n",
    "    \"labels\": train_labels_numeric,  # Corresponding numeric labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1387.31 examples/s]\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Apply tokenization and label alignment\n",
    "train_dataset = train_dataset.map(lambda e: tokenize_and_align_labels(tokenizer, e['tokens'], e['labels']), batched=True)\n",
    "\n",
    "# Step 10: Initialize the model (with label2id and id2label)\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohan.joydhar\\AppData\\Local\\miniconda3\\envs\\ner-env\\Lib\\site-packages\\transformers\\training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Define the TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    #per_device_train_batch_size=8,\n",
    "    #per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    #weight_decay=0.01\n",
    ")\n",
    "\n",
    "\n",
    "# Step 13: Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the model to be trained\n",
    "    args=training_args,                  # training arguments\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=train_dataset,          # evaluation dataset (for demonstration purposes, using train for eval)      # evaluation metrics function\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 10%|â–ˆ         | 2/20 [00:03<00:26,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27185097336769104, 'eval_runtime': 0.4943, 'eval_samples_per_second': 18.206, 'eval_steps_per_second': 4.046, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 20%|â–ˆâ–ˆ        | 4/20 [00:07<00:25,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2218555212020874, 'eval_runtime': 0.6934, 'eval_samples_per_second': 12.979, 'eval_steps_per_second': 2.884, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:11<00:24,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1900850087404251, 'eval_runtime': 0.7035, 'eval_samples_per_second': 12.794, 'eval_steps_per_second': 2.843, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:15<00:22,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1614873707294464, 'eval_runtime': 0.6841, 'eval_samples_per_second': 13.156, 'eval_steps_per_second': 2.923, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:20<00:19,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14277811348438263, 'eval_runtime': 0.8888, 'eval_samples_per_second': 10.127, 'eval_steps_per_second': 2.25, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:25<00:16,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13137286901474, 'eval_runtime': 0.7811, 'eval_samples_per_second': 11.522, 'eval_steps_per_second': 2.56, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:29<00:12,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1221974790096283, 'eval_runtime': 0.7307, 'eval_samples_per_second': 12.317, 'eval_steps_per_second': 2.737, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:34<00:08,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11575402319431305, 'eval_runtime': 0.9749, 'eval_samples_per_second': 9.232, 'eval_steps_per_second': 2.051, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:40<00:04,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1110372468829155, 'eval_runtime': 1.0243, 'eval_samples_per_second': 8.786, 'eval_steps_per_second': 1.952, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:48<00:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10895165055990219, 'eval_runtime': 1.111, 'eval_samples_per_second': 8.101, 'eval_steps_per_second': 1.8, 'epoch': 10.0}\n",
      "{'train_runtime': 48.9154, 'train_samples_per_second': 1.84, 'train_steps_per_second': 0.409, 'train_loss': 0.21605362892150878, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=0.21605362892150878, metrics={'train_runtime': 48.9154, 'train_samples_per_second': 1.84, 'train_steps_per_second': 0.409, 'total_flos': 3628718598780.0, 'train_loss': 0.21605362892150878, 'epoch': 10.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 14: Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where you want to save the model and tokenizer\n",
    "output_dir = \"model\"  # Replace this with the desired output directory\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "# Save the tokenizer (this is necessary for later use)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {output_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at model and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved as model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load your model and tokenizer\n",
    "model = AutoModel.from_pretrained(\"model\")  # Replace \"model\" with your model's directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model\")\n",
    "\n",
    "# Save the model and tokenizer to a .pkl file\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump((model, tokenizer), f)\n",
    "\n",
    "print(\"Model and tokenizer saved as model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Load the trained model and tokenizer\n",
    "model_path = \"model\"  # Replace this with the path where your model is saved\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  # Use the same tokenizer as used during training\n",
    "model = BertForTokenClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the label mapping (you should have the same id2label and label2id as in training)\n",
    "id2label = {0: \"O\", 1: \"B-organic-chemicals\", 2: \"I-organic-chemicals\", 3: \"B-catalyst\", 4: \"I-catalyst\"}  # Update this if you have more classes\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "# 3. Initialize the NER pipeline\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=-1)  # `device=-1` means CPU; change to `0` for GPU\n",
    "\n",
    "# 5. Tokenize and make predictions\n",
    "def predict_entities(text):\n",
    "    # Use the NER pipeline for prediction\n",
    "    ner_results = nlp(text)\n",
    "    \n",
    "    # Post-process results: match predicted tokens with labels and tokens\n",
    "    predictions = []\n",
    "    for result in ner_results:\n",
    "        entity = result['word']\n",
    "        label = result['entity']\n",
    "        score = result['score']\n",
    "        predictions.append({\"entity\": entity, \"label\": label, \"score\": score})\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_subwords(predictions):\n",
    "    \"\"\"\n",
    "    Merges subword tokens and keeps only the first subword's label for each word.\n",
    "    \"\"\"\n",
    "    merged_predictions = []\n",
    "    current_word = ''\n",
    "    current_label = None\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        token = prediction['entity']\n",
    "        label = prediction['label']\n",
    "        score = prediction['score']\n",
    "        \n",
    "        # Check if the token is a continuation of the previous one (subword token)\n",
    "        if token.startswith('##'):\n",
    "            # If it's a subword token, add it to the current word\n",
    "            current_word += token[2:]  # Remove the '##' part of the subword token\n",
    "        else:\n",
    "            # If it's the first token of a new word, save the previous word and start a new one\n",
    "            if current_word:\n",
    "                merged_predictions.append({'entity': current_word, 'label': current_label, 'score': score})\n",
    "            current_word = token  # Start a new word\n",
    "            current_label = label  # Set the label for this word\n",
    "            # No score adjustment here, use the score of the first subword\n",
    "\n",
    "    # Append the last word\n",
    "    if current_word:\n",
    "        merged_predictions.append({'entity': current_word, 'label': current_label, 'score': score})\n",
    "\n",
    "    return merged_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbomer refers to a family of synthetic, high-molecular-weight polymers made from acrylic acid as the primary monomer, with small amounts of polyalkenyl polyethers acting as crosslinkers. These polymers are widely synthesized through free-radical polymerization, typically in an aqueous solution or solvent-based systems. The ideal temperature for polymerization is between 70-80Â°C, where a free-radical initiator (such as benzoyl peroxide or azobisisobutyronitrile (AIBN)) acts as the catalyst to initiate the polymerization reaction. These crosslinked polyacrylate structures are highly hydrophilic, allowing carbomer to swell significantly in water, forming viscous gels even at low concentrations.\n",
      "Predictions: [{'entity': 'bomer', 'label': None, 'score': np.float32(0.33687964)}, {'entity': 'acrylic', 'label': 'B-organic-chemicals', 'score': np.float32(0.2837343)}, {'entity': 'acidkeneth', 'label': 'B-catalyst', 'score': np.float32(0.45214137)}, {'entity': '70', 'label': 'B-property values', 'score': np.float32(0.45839557)}, {'entity': '80Â°c', 'label': 'B-property values', 'score': np.float32(0.3641449)}, {'entity': 'benzoy', 'label': 'B-organic-chemicals', 'score': np.float32(0.25380862)}, {'entity': 'peroxide', 'label': 'B-catalyst', 'score': np.float32(0.28705817)}, {'entity': 'azobisibuit', 'label': 'B-organic-chemicals', 'score': np.float32(0.25389308)}, {'entity': 'aibnac', 'label': 'B-organic-chemicals', 'score': np.float32(0.33960706)}]\n"
     ]
    }
   ],
   "source": [
    "# 4. Example Input Text\n",
    "text = input(\"Enter text\")\n",
    "# 6. Predict entities\n",
    "predicted_entities = predict_entities(text)\n",
    "\n",
    "# Merge subword tokens and clean the predictions\n",
    "merged_predictions = merge_subwords(predicted_entities)\n",
    "# 7. Print the results\n",
    "print(text)\n",
    "print(\"Predictions:\", merged_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
